# -*- coding: utf-8 -*-
"""Copy of rp1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13PABWI3Ph5zKt3nMHbGLi4Zo2rpwTnJk
"""

# !wget 'https://nemar.org/dataexplorer/download?filepath=/data/nemar/openneuro//zip_files/ds004504.zip'
# !unzip /content/download?filepath=%2Fdata%2Fnemar%2Fopenneuro%2F%2Fzip_files%2Fds004504.zip

# !pip install python-multipart

# !pip install mne

import mne
import base64
import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import scipy.io
from mne.time_frequency import tfr_array_morlet
from scipy.stats import skew
from scipy.stats import kurtosis
from scipy.stats import entropy
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Softmax, Dropout, Bidirectional
from sklearn.decomposition import PCA
import sklearn.model_selection
import pickle
from tensorflow.keras.models import load_model
from fastapi import FastAPI
from pydantic import BaseModel
import json
import uvicorn
from pyngrok import ngrok
from fastapi.middleware.cors import CORSMiddleware
import nest_asyncio
import json
from fastapi import File, UploadFile
from fastapi.responses import FileResponse





# !pip freeze

# def prepare_data_training(path):
#     raw = mne.io.read_raw_eeglab(path, preload=True)
#     tmp = path[23]+path[24]
#     person = int(tmp)
#     label = 0
#     if person <= 36:
#       label = 1
#     raw.drop_channels(['Fz', 'Pz', 'Cz'])
#     #EPOCHING
#     epochs = mne.make_fixed_length_epochs(raw, duration=2, overlap = 1, preload=False)
#     data = epochs.get_data()
#     #FILTERING
#     data = mne.filter.filter_data(data, raw.info['sfreq'], 0.5, 45)
#     label_array = np.full(data.shape[0], label)
#     #SCALING
#     scaler_object = mne.decoding.Scaler(info = raw.info)
#     scaler_object.fit(data)
#     data = scaler_object.transform(data)
#     #PCA
#     data_2d = np.reshape(data, (data.shape[0]*data.shape[1], -1))
#     pca = PCA(n_components = 100)
#     data_2d_pca = pca.fit_transform(data_2d)
#     data = np.reshape(data_2d_pca, (data.shape[0], data.shape[1], -1))
#     data = np.transpose(data, (0, 2, 1))
#     return data, label_array

# def prepare_data_testing(path):
#     raw = mne.io.read_raw_eeglab(path, preload=True)
#     raw.drop_channels(['Fz', 'Pz', 'Cz'])
#     #EPOCHING
#     epochs = mne.make_fixed_length_epochs(raw, duration=2, overlap = 1, preload=False)
#     data = epochs.get_data()
#     #FILTERING
#     data = mne.filter.filter_data(data, raw.info['sfreq'], 0.5, 45)
#     #SCALING
#     scaler_object = mne.decoding.Scaler(info = raw.info)
#     scaler_object.fit(data)
#     data = scaler_object.transform(data)
#     #PCA
#     data_2d = np.reshape(data, (data.shape[0]*data.shape[1], -1))
#     pca = PCA(n_components = 100)
#     data_2d_pca = pca.fit_transform(data_2d)
#     data = np.reshape(data_2d_pca, (data.shape[0], data.shape[1], -1))
#     data = np.transpose(data, (0, 2, 1))
#     return data

# #DATA PREPARATION
# main_path = "/content/ds004504"
# x_data = np.empty((0, 100, 16))
# y_data = np.empty((0))
# for folder in os.listdir(main_path):
#   if folder[0] == 's' and folder != 'sub-050' and folder != 'sub-051' and folder != 'sub-052' and folder != 'sub-053' and folder != 'sub-054' and folder != 'sub-010' and folder != 'sub-011' and folder != 'sub-012' and folder != 'sub-013' and folder != 'sub-014' and int(folder[5]+folder[6]) <= 65:
#     path = main_path + '/' + folder + '/eeg/' + folder + '_task-eyesclosed_eeg.set'
#     x_data_i, y_data_i = prepare_data_training(path)
#     x_data = np.concatenate((x_data, x_data_i), axis = 0)
#     y_data = np.concatenate((y_data, y_data_i), axis = 0)

# print(x_data.shape)
# print(y_data.shape)

# x_data_train, x_data_val, y_data_train, y_data_val = sklearn.model_selection.train_test_split(x_data, y_data, train_size = 0.60, random_state=42, shuffle=True)
# #MODEL
# model = keras.Sequential()
# model.add(keras.Input(shape = (100, 16), name='input'))
# model.add(LSTM(8, return_sequences = True, name = 'lstm1'))
# model.add(Dropout(0.2, name='dropout1'))
# model.add(LSTM(8, name = 'lstm2'))
# model.add(Dropout(0.2, name='dropout2'))
# model.add(Dense(2, name='dense'))
# model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),
#     optimizer = 'adam', metrics = ['accuracy'])
# model.summary()

# history = model.fit(x_data_train, y_data_train, epochs = 5, validation_data = (x_data_val, y_data_val))
# print(history)

# model = model.load_weights('weights')

def prepare_data_testing(path):
    if path.endswith('.edf'):
      raw = mne.io.read_raw_edf(path, preload=True)
    elif path.endswith('.set'):
      raw = mne.io.read_raw_eeglab(path, preload=True)
    if 'FZ-AVG' in raw.ch_names:
      raw.drop_channels(['FZ-AVG'])
    elif 'Fz' in raw.ch_names:
      raw.drop_channels(['Fz'])
    if 'PZ-AVG' in raw.ch_names:
      raw.drop_channels(['PZ-AVG'])
    elif 'Pz' in raw.ch_names:
      raw.drop_channels(['Pz'])
    if 'CZ-AVG' in raw.ch_names:
      raw.drop_channels(['CZ-AVG'])
    elif 'Cz' in raw.ch_names:
      raw.drop_channels(['Cz'])
    if 'EKG' in raw.ch_names:
      raw.drop_channels(['EKG'])
    if 'Photic' in raw.ch_names:
      raw.drop_channels(['Photic'])
    #EPOCHING
    epochs = mne.make_fixed_length_epochs(raw, duration=5, overlap = 2.5, preload=False)
    data = epochs.get_data()
    #FILTERING
    data = mne.filter.filter_data(data, raw.info['sfreq'], 0.5, 45)
    #SCALING
    scaler_object = mne.decoding.Scaler(info = raw.info)
    scaler_object.fit(data)
    data = scaler_object.transform(data)
    #PCA
    data_2d = np.reshape(data, (data.shape[0]*data.shape[1], -1))
    pca = PCA(n_components = 100)
    data_2d_pca = pca.fit_transform(data_2d)
    data = np.reshape(data_2d_pca, (data.shape[0], data.shape[1], -1))
    data = np.transpose(data, (0, 2, 1))
    return data

model = tf.keras.models.load_model('rp1_model.keras')

def gen_plot(path_test):
  if path_test.endswith('.edf'):
    raw = mne.io.read_raw_edf(path_test, preload=True)
  elif path_test.endswith('.set'):
    raw = mne.io.read_raw_eeglab(path_test, preload=True)
  data = raw.get_data()
  data_mean = np.mean(data, axis=0)
  plt.figure(figsize=(8,4), facecolor='xkcd:charcoal grey')
  plt.plot(data_mean, color="xkcd:orangey yellow")
  plt.axis('off')
  plt.xlim(0, data_mean.shape[0]/2)
  plt.savefig("working/plot.png")
  return "working/plot.png"

def make_prediction(path_test):
  data_test= prepare_data_testing(path_test)
  y_pred = np.argmax(model.predict(data_test), axis=1)
  mean = np.mean(y_pred)
  if mean < 0.5:
    return {"prediction": f"Alzheimer's Negative with confidence: {(1-mean)*100: .2f}%"}
  else:
    return {"prediction": f"Alzheimer's Positive with confidence: {mean*100: .2f}%"}



# #TESTING ON 10 SUBJECTS
# predictions = []
# confidence_scores = []
# l = [10, 11, 12, 13, 14, 50, 51, 52, 53, 54]
# for i in l:
#   path_test = '/content/ds004504/sub-0' + str(i) + '/eeg/sub-0' + str(i) + '_task-eyesclosed_eeg.set'
#   data_test = prepare_data_testing(path_test)
#   y_pred = np.argmax(model.predict(data_test), axis=1)
#   mean = np.mean(y_pred)
#   confidence_scores.append(int(100*mean))
#   if mean < 0.5:
#     predictions.append("CN")
#   else:
#     predictions.append("AD")

# print(predictions)
# print(confidence_scores)

# import pickle
# filename = 'finalized_model.sav'
# pickle.dump(model, open(filename, 'wb'))
# loaded_model = pickle.load(open(filename, 'rb'))

# !pip install fastapi
# !pip install uvicorn
# !pip install pickle5
# !pip install pydantic
# !pip install scikit-learn
# !pip install requests
# !pip install pypi-json
# !pip install pyngrok
# !pip install nest-asyncio

# from fastapi import FastAPI
# from pydantic import BaseModel
# import pickle
# import json
# import uvicorn
# from pyngrok import ngrok
# from fastapi.middleware.cors import CORSMiddleware
# import nest_asyncio

# app = FastAPI()

# origins = ["http://localhost:3000"]


# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=origins,
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# !pip install python-multipart

# from fastapi import FastAPI, File, UploadFile
# import os
# from typing import Annotated

app = FastAPI()

@app.get('/')
def index():
    return {'message': 'This is the homepage of the API shshsh'}

# Define the directory where uploaded files will be saved
UPLOAD_DIR = "/app/working/"

# Create the upload directory if it doesn't exist
os.makedirs(UPLOAD_DIR, exist_ok=True)

@app.post("/pred")
def get_pred(file: UploadFile = File(...)):
    try:
        contents = file.file.read()
        with open(file.filename, 'wb') as f:
            f.write(contents)
    except Exception:
        return {"message": "There was an error uploading the file"}
    finally:
        file.file.close()
    file_path = file.filename
    pred_name = make_prediction(file_path)
    plot_path = gen_plot(file_path)
    with open(plot_path, "rb") as image_file:
      image_data = image_file.read()

    encoded_image = base64.b64encode(image_data).decode("utf-8")
    return {'prediction': pred_name, 'image': encoded_image}







